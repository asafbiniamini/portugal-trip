"""
Surfline Forecast Scraper
××©×•×š ×ª×—×–×™×•×ª ×’×œ×™× ×-Surfline ×¢×‘×•×¨ ×¡×¤×•×˜×™× ×‘×¤×•×¨×˜×•×’×œ
×’×™×©×” ×™×©×™×¨×” ×œ-API ×œ×œ× ×”×ª×—×‘×¨×•×ª (×ª×—×–×™×ª ×©×‘×•×¢ ×–××™× ×” ×œ×›×•×œ×)
"""

import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime, timedelta
import time
import re

# ×¨×©×™××ª ×¡×¤×•×˜×™× ×‘×¤×•×¨×˜×•×’×œ - ×“×¨×•× (Algarve) ×•××¨×›×– (Peniche/Ericeira)
PORTUGAL_SPOTS = {
    # ×“×¨×•× ×¤×•×¨×˜×•×’×œ (Algarve)
    "Arrifana": {
        "name": "Arrifana",
        "location": "Aljezur, Costa Vicentina",
        "region": "×“×¨×•× (Algarve)",
        "surfline_id": "5842041f4e65fad6a7708b87",
        "surfline_url": "https://www.surfline.com/surf-report/arrifana/5842041f4e65fad6a7708b87"
    },
    "Tonel": {
        "name": "Tonel",
        "location": "Sagres, Algarve",
        "region": "×“×¨×•× (Algarve)",
        "surfline_id": "5842041f4e65fad6a7708b88",
        "surfline_url": "https://www.surfline.com/surf-report/tonel/5842041f4e65fad6a7708b88"
    },
    "Lagos": {
        "name": "Lagos",
        "location": "Lagos, Algarve",
        "region": "×“×¨×•× (Algarve)",
        "surfline_id": "5842041f4e65fad6a7708b89",
        "surfline_url": "https://www.surfline.com/surf-report/lagos/5842041f4e65fad6a7708b89"
    },
    "Praia do Amado": {
        "name": "Praia do Amado",
        "location": "Carrapateira, Costa Vicentina",
        "region": "×“×¨×•× (Algarve)",
        "surfline_id": "5842041f4e65fad6a7708b90",
        "surfline_url": "https://www.surfline.com/surf-report/praia-do-amado/5842041f4e65fad6a7708b90"
    },
    # ××¨×›×– ×¤×•×¨×˜×•×’×œ - ×¤× ×™×©
    "Supertubos": {
        "name": "Supertubos",
        "location": "Peniche, Central Portugal",
        "region": "××¨×›×– (Peniche)",
        "surfline_id": None,  # ×™×™××¦× ××•×˜×•××˜×™×ª
        "search_terms": ["Supertubos", "Peniche", "Portugal"]
    },
    "Baleal": {
        "name": "Baleal",
        "location": "Peniche, Central Portugal",
        "region": "××¨×›×– (Peniche)",
        "surfline_id": None,
        "search_terms": ["Baleal", "Peniche", "Portugal"]
    },
    # ××¨×›×– ×¤×•×¨×˜×•×’×œ - ××¨×™×¡×™×™×¨×”
    "Ribeira d'Ilhas": {
        "name": "Ribeira d'Ilhas",
        "location": "Ericeira, Central Portugal",
        "region": "××¨×›×– (Ericeira)",
        "surfline_id": None,
        "search_terms": ["Ribeira d'Ilhas", "Ericeira", "Portugal"]
    },
    "Praia do Sul": {
        "name": "Praia do Sul",
        "location": "Ericeira, Central Portugal",
        "region": "××¨×›×– (Ericeira)",
        "surfline_id": None,
        "search_terms": ["Praia do Sul", "Ericeira", "Portugal"]
    }
}

def create_session():
    """
    ×™×¦×™×¨×ª session ×¢× headers ××ª××™××™×
    """
    session = requests.Session()
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'application/json, text/plain, */*',
        'Accept-Language': 'en-US,en;q=0.9',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Referer': 'https://www.surfline.com/',
        'Origin': 'https://www.surfline.com'
    })
    return session

def find_spot_id(spot_name, session):
    """
    ××¦× Spot ID ×œ×¤×™ ×©× (×× ×œ× ×™×“×•×¢)
    """
    search_url = "https://services.surfline.com/kbyg/spots/search"
    params = {"q": spot_name, "querySize": 10}
    
    try:
        response = session.get(search_url, params=params, timeout=10)
        if response.status_code == 200:
            results = response.json()
            if results and len(results) > 0:
                # ×—×¤×© ×ª×•×¦××” ×‘×¤×•×¨×˜×•×’×œ
                for result in results:
                    name = result.get('name', '').lower()
                    location = result.get('location', {}).get('name', '').lower()
                    country = result.get('location', {}).get('country', '').lower()
                    
                    # ×•×“× ×©×–×” ×‘×¤×•×¨×˜×•×’×œ
                    if 'portugal' in country or 'portugal' in location:
                        # ×‘×“×•×§ ×× ×”×©× ×ª×•××
                        if any(term.lower() in name or term.lower() in location for term in spot_name.split()):
                            return result.get('_id')
                
                # ×× ×œ× ××¦×× ×• ×”×ª×××” ××“×•×™×§×ª, × ×—×–×™×¨ ××ª ×”×¨××©×•×Ÿ ×‘×¤×•×¨×˜×•×’×œ
                for result in results:
                    country = result.get('location', {}).get('country', '').lower()
                    if 'portugal' in country:
                        return result.get('_id')
                
                # ×× ××™×Ÿ ×ª×•×¦××•×ª ×‘×¤×•×¨×˜×•×’×œ, × ×—×–×™×¨ ××ª ×”×¨××©×•×Ÿ
                return results[0].get('_id')
    except Exception as e:
        pass  # ×œ× × ×“×¤×™×¡ ×©×’×™××” ×›××Ÿ ×›×“×™ ×œ× ×œ×”×¦×™×£
    return None

def get_forecast_via_api(spot_id, session):
    """
    ××©×•×š ×ª×—×–×™×ª ×“×¨×š Surfline API (×œ×œ× ×”×ª×—×‘×¨×•×ª - ×ª×—×–×™×ª ×©×‘×•×¢)
    """
    # Surfline API endpoints
    base_url = "https://services.surfline.com/kbyg"
    
    forecasts = {}
    
    # 1. Wave forecast
    wave_url = f"{base_url}/spots/forecasts/wave"
    wave_params = {
        "spotId": spot_id,
        "days": 7,
        "intervalHours": 1
    }
    
    try:
        response = session.get(wave_url, params=wave_params, timeout=10)
        if response.status_code == 200:
            forecasts['wave'] = response.json()
        else:
            print(f"  âš ï¸  Wave API: {response.status_code}")
    except Exception as e:
        print(f"  âš ï¸  Wave API error: {e}")
    
    # 2. Wind forecast
    wind_url = f"{base_url}/spots/forecasts/wind"
    wind_params = {
        "spotId": spot_id,
        "days": 7,
        "intervalHours": 1
    }
    
    try:
        response = session.get(wind_url, params=wind_params, timeout=10)
        if response.status_code == 200:
            forecasts['wind'] = response.json()
    except Exception as e:
        print(f"  âš ï¸  Wind API error: {e}")
    
    # 3. Conditions forecast
    conditions_url = f"{base_url}/spots/forecasts/conditions"
    conditions_params = {
        "spotId": spot_id,
        "days": 7
    }
    
    try:
        response = session.get(conditions_url, params=conditions_params, timeout=10)
        if response.status_code == 200:
            forecasts['conditions'] = response.json()
    except Exception as e:
        print(f"  âš ï¸  Conditions API error: {e}")
    
    # 4. Tide forecast
    tide_url = f"{base_url}/spots/forecasts/tides"
    tide_params = {
        "spotId": spot_id,
        "days": 7
    }
    
    try:
        response = session.get(tide_url, params=tide_params, timeout=10)
        if response.status_code == 200:
            forecasts['tide'] = response.json()
    except Exception as e:
        print(f"  âš ï¸  Tide API error: {e}")
    
    return forecasts if forecasts else None

def analyze_forecast(forecast_data, spot_name, dates_range):
    """
    × ×™×ª×•×— ×ª×—×–×™×ª ×•×”××œ×¦×•×ª ×¢×‘×•×¨ ×”×ª××¨×™×›×™× 13-20 ×‘× ×•×‘××‘×¨
    """
    recommendations = {
        "spot": spot_name,
        "best_days": [],
        "summary": "",
        "wave_heights": [],
        "wind_conditions": [],
        "overall_rating": ""
    }
    
    if not forecast_data or 'wave' not in forecast_data:
        return recommendations
    
    try:
        wave_data = forecast_data['wave']
        
        # ×ª××¨×™×›×™×: 13-20 ×‘× ×•×‘××‘×¨ 2025
        target_dates = []
        for i in range(8):  # 13-20 = 8 ×™××™×
            date = datetime(2025, 11, 13) + timedelta(days=i)
            target_dates.append(date.strftime("%Y-%m-%d"))
        
        # × ×™×ª×•×— × ×ª×•× ×™ ×’×œ×™×
        if 'data' in wave_data and 'wave' in wave_data['data']:
            wave_points = wave_data['data']['wave']
            
            daily_max = {}
            for point in wave_points:
                timestamp = point.get('timestamp', 0)
                dt = datetime.fromtimestamp(timestamp)
                date_str = dt.strftime("%Y-%m-%d")
                
                if date_str in target_dates:
                    surf_min = point.get('surf', {}).get('min', 0)
                    surf_max = point.get('surf', {}).get('max', 0)
                    
                    if date_str not in daily_max:
                        daily_max[date_str] = {'min': surf_min, 'max': surf_max, 'count': 0}
                    else:
                        daily_max[date_str]['max'] = max(daily_max[date_str]['max'], surf_max)
                        daily_max[date_str]['min'] = min(daily_max[date_str]['min'], surf_min)
                        daily_max[date_str]['count'] += 1
            
            # ××¦×™××ª ×”×™××™× ×”×˜×•×‘×™× ×‘×™×•×ª×¨
            for date_str in target_dates:
                if date_str in daily_max:
                    avg_height = (daily_max[date_str]['min'] + daily_max[date_str]['max']) / 2
                    recommendations['wave_heights'].append({
                        'date': date_str,
                        'min': daily_max[date_str]['min'],
                        'max': daily_max[date_str]['max'],
                        'avg': avg_height
                    })
                    
                    # ×”×•×¡×£ ××ª ×›×œ ×”×™××™× (×’× ×§×˜× ×™×) ×œ×”××œ×¦×•×ª
                    recommendations['best_days'].append({
                        'date': date_str,
                        'height': avg_height,
                        'min': daily_max[date_str]['min'],
                        'max': daily_max[date_str]['max']
                    })
            
            # ××™×•×Ÿ ×œ×¤×™ ×’×•×‘×” ×’×œ×™× (××”×’×“×•×œ ×œ×§×˜×Ÿ)
            recommendations['best_days'].sort(key=lambda x: x['height'], reverse=True)
            
            # ×¡×™×›×•× ×•×”×¢×¨×›×”
            if recommendations['best_days']:
                best_day = recommendations['best_days'][0]
                max_height = best_day['max']
                
                # ×”×¢×¨×›×ª ××™×›×•×ª ×œ×¤×™ ×’×•×‘×” ××§×¡×™××œ×™
                if max_height >= 2.0:
                    rating = "â­â­â­â­â­"
                    quality = "××¦×•×™×Ÿ"
                elif max_height >= 1.5:
                    rating = "â­â­â­â­"
                    quality = "×˜×•×‘ ×××•×“"
                elif max_height >= 1.0:
                    rating = "â­â­â­"
                    quality = "×˜×•×‘"
                elif max_height >= 0.6:
                    rating = "â­â­"
                    quality = "×‘×™× ×•× ×™ - ×’×œ×™× ×§×˜× ×™×"
                else:
                    rating = "â­"
                    quality = "×§×˜× ×™× ×××•×“"
                
                recommendations['summary'] = f"×”×™×•× ×”×˜×•×‘ ×‘×™×•×ª×¨: {best_day['date']} - ×’×œ×™× {best_day['min']:.1f}-{best_day['max']:.1f}m ({quality})"
                recommendations['overall_rating'] = rating
            else:
                recommendations['summary'] = "××™×Ÿ × ×ª×•× ×™× ×–××™× ×™×"
                recommendations['overall_rating'] = "â“"
        
    except Exception as e:
        print(f"  âš ï¸  Error analyzing forecast: {e}")
    
    return recommendations

def main():
    """
    ×¤×•× ×§×¦×™×” ×¨××©×™×ª - ××©×™×›×ª ×ª×—×–×™×•×ª ×œ×œ× ×”×ª×—×‘×¨×•×ª
    """
    print("ğŸŒŠ Surfline Forecast Scraper - ×¤×•×¨×˜×•×’×œ 13-20 ×‘× ×•×‘××‘×¨")
    print("=" * 60)
    print("××©×™×›×ª ×ª×—×–×™×•×ª ×©×‘×•×¢×™×•×ª (×–××™×Ÿ ×œ×›×•×œ× ×œ×œ× ×”×ª×—×‘×¨×•×ª)")
    print("=" * 60)
    
    # ×™×¦×™×¨×ª session
    session = create_session()
    
    # ××©×™×›×ª ×ª×—×–×™×•×ª ×œ×›×œ ×”×¡×¤×•×˜×™×
    print("\nğŸ“Š ××•×©×š ×ª×—×–×™×•×ª ×-Surfline...")
    all_forecasts = {}
    all_recommendations = []
    
    for spot_key, spot_info in PORTUGAL_SPOTS.items():
        print(f"\nğŸ“ ×‘×•×“×§ {spot_info['name']} ({spot_info['location']}) - {spot_info['region']}...")
        
        # ×× ×”-ID ×œ× ×™×“×•×¢, × ×¡×” ×œ××¦×•× ××•×ª×•
        spot_id = spot_info.get('surfline_id')
        if not spot_id:
            print(f"   ğŸ” ××—×¤×© Spot ID...")
            # × ×¡×” ×¢× ×©××•×ª ×—×™×¤×•×© ×©×•× ×™×
            search_terms = spot_info.get('search_terms', [spot_info['name']])
            found_id = None
            for term in search_terms:
                found_id = find_spot_id(term, session)
                if found_id:
                    print(f"   âœ… ××¦××ª×™ Spot ID ×¢× '{term}': {found_id}")
                    break
            if found_id:
                spot_id = found_id
            else:
                print(f"   âš ï¸  ×œ× ××¦××ª×™ Spot ID, ××“×œ×’...")
                continue
        else:
            print(f"   Spot ID: {spot_id}")
        
        # ××©×™×›×ª ×ª×—×–×™×ª ×“×¨×š API
        forecast = get_forecast_via_api(spot_id, session)
        
        if forecast:
            all_forecasts[spot_key] = forecast
            print(f"   âœ… ×§×™×‘×œ×ª×™ ×ª×—×–×™×ª!")
            
            # × ×™×ª×•×— ×•×”××œ×¦×•×ª
            recommendation = analyze_forecast(forecast, spot_info['name'], "2025-11-13 to 2025-11-20")
            recommendation['location'] = spot_info['location']
            recommendation['region'] = spot_info['region']
            all_recommendations.append(recommendation)
            
            # ×”×“×¤×¡×ª ×¡×™×›×•× ××”×™×¨
            if recommendation['best_days']:
                best = recommendation['best_days'][0]
                print(f"   ğŸŒŠ ×”×™×•× ×”×˜×•×‘ ×‘×™×•×ª×¨: {best['date']} - ×’×œ×™× {best['min']:.1f}-{best['max']:.1f}m")
            else:
                print(f"   âš ï¸  ××™×Ÿ × ×ª×•× ×™× ×–××™× ×™×")
        else:
            print(f"   âŒ ×œ× ×”×¦×œ×—×ª×™ ×œ×§×‘×œ ×ª×—×–×™×ª")
        
        # ×”××ª× ×” ×§×¦×¨×” ×‘×™×Ÿ ×‘×§×©×•×ª
        time.sleep(1)
    
    # ×©××™×¨×ª ×ª×•×¦××•×ª ×’×•×œ××™×•×ª
    output_file = "surfline_forecasts_raw.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(all_forecasts, f, ensure_ascii=False, indent=2)
    print(f"\nğŸ’¾ × ×ª×•× ×™× ×’×•×œ××™×™× × ×©××¨×• ×‘-{output_file}")
    
    # ×©××™×¨×ª ×”××œ×¦×•×ª
    recommendations_file = "surfline_recommendations.json"
    with open(recommendations_file, 'w', encoding='utf-8') as f:
        json.dump(all_recommendations, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ ×”××œ×¦×•×ª × ×©××¨×• ×‘-{recommendations_file}")
    
    # ×”×“×¤×¡×ª ×¡×™×›×•× ×”××œ×¦×•×ª
    print("\n" + "=" * 60)
    print("ğŸ“ˆ ×¡×™×›×•× ×”××œ×¦×•×ª ×œ×©×‘×•×¢ 13-20 ×‘× ×•×‘××‘×¨:")
    print("=" * 60)
    
    # ××™×•×Ÿ ×œ×¤×™ ×’×•×‘×” ××§×¡×™××œ×™
    sorted_recs = sorted(all_recommendations, key=lambda x: x['best_days'][0]['max'] if x['best_days'] else 0, reverse=True)
    
    # ×”×¤×¨×“×” ×œ×¤×™ ××–×•×¨×™×
    south_spots = [r for r in sorted_recs if '×“×¨×•×' in r.get('region', '')]
    central_spots = [r for r in sorted_recs if '××¨×›×–' in r.get('region', '')]
    
    print("\n" + "=" * 60)
    print("ğŸ„ ×“×¨×•× ×¤×•×¨×˜×•×’×œ (Algarve):")
    print("=" * 60)
    for rec in south_spots:
        print(f"\nğŸ“ {rec['spot']} ({rec['location']})")
        print(f"   {rec['overall_rating']} {rec['summary']}")
        if rec['best_days']:
            best = rec['best_days'][0]
            print(f"   ğŸŒŠ ×”×™×•× ×”×˜×•×‘ ×‘×™×•×ª×¨: {best['date']} - ×’×œ×™× {best['min']:.1f}-{best['max']:.1f}m")
    
    print("\n" + "=" * 60)
    print("ğŸ„ ××¨×›×– ×¤×•×¨×˜×•×’×œ (Peniche/Ericeira):")
    print("=" * 60)
    for rec in central_spots:
        print(f"\nğŸ“ {rec['spot']} ({rec['location']})")
        print(f"   {rec['overall_rating']} {rec['summary']}")
        if rec['best_days']:
            best = rec['best_days'][0]
            print(f"   ğŸŒŠ ×”×™×•× ×”×˜×•×‘ ×‘×™×•×ª×¨: {best['date']} - ×’×œ×™× {best['min']:.1f}-{best['max']:.1f}m")
    
    # ×”××œ×¦×” ×¡×•×¤×™×ª
    print("\n" + "=" * 60)
    print("ğŸ’¡ ×”××œ×¦×” ×¡×•×¤×™×ª:")
    print("=" * 60)
    
    if sorted_recs and sorted_recs[0]['best_days']:
        best_spot = sorted_recs[0]
        best_day = best_spot['best_days'][0]
        print(f"\nğŸ† ×”×¡×¤×•×˜ ×”×˜×•×‘ ×‘×™×•×ª×¨: {best_spot['spot']} ({best_spot['region']})")
        print(f"   ğŸ“… ×ª××¨×™×š ××•××œ×¥: {best_day['date']}")
        print(f"   ğŸŒŠ ×’×œ×™×: {best_day['min']:.1f}-{best_day['max']:.1f}m")
        print(f"   ğŸ“ ××™×§×•×: {best_spot['location']}")
        
        # ×”×©×•×•××” ×‘×™×Ÿ ××–×•×¨×™×
        if south_spots and central_spots:
            south_max = max([s['best_days'][0]['max'] for s in south_spots if s['best_days']], default=0)
            central_max = max([s['best_days'][0]['max'] for s in central_spots if s['best_days']], default=0)
            
            print(f"\nğŸ“Š ×”×©×•×•××” ×‘×™×Ÿ ××–×•×¨×™×:")
            print(f"   ×“×¨×•× (Algarve): ×’×œ×™× ×¢×“ {south_max:.1f}m")
            print(f"   ××¨×›×– (Peniche/Ericeira): ×’×œ×™× ×¢×“ {central_max:.1f}m")
            
            if central_max > south_max:
                print(f"\nâœ… ×”××œ×¦×”: ××¨×›×– ×¤×•×¨×˜×•×’×œ (Peniche/Ericeira) ××¦×™×¢ ×’×œ×™× ×’×“×•×œ×™× ×™×•×ª×¨!")
            elif south_max > central_max:
                print(f"\nâœ… ×”××œ×¦×”: ×“×¨×•× ×¤×•×¨×˜×•×’×œ (Algarve) ××¦×™×¢ ×’×œ×™× ×’×“×•×œ×™× ×™×•×ª×¨!")
            else:
                print(f"\nâœ… ×©× ×™ ×”××–×•×¨×™× ××¦×™×¢×™× ×’×œ×™× ×“×•××™× - ×‘×—×¨ ×œ×¤×™ ×”×¢×“×¤×•×ª × ×•×¡×¤×•×ª")
    
    print("\n" + "=" * 60)
    print("ğŸ“… ×¤×™×¨×•×˜ ××œ× ×œ×›×œ ×”×¡×¤×•×˜×™×:")
    print("=" * 60)
    for rec in sorted_recs:
        print(f"\nğŸ„ {rec['spot']} ({rec['location']}) - {rec.get('region', '')}")
        print(f"   {rec['overall_rating']} {rec['summary']}")
        if rec['best_days']:
            print(f"   ğŸ“… ×›×œ ×”×™××™× (13-20 ×‘× ×•×‘××‘×¨):")
            for day in rec['best_days']:
                print(f"      â€¢ {day['date']}: ×’×œ×™× {day['min']:.1f}-{day['max']:.1f}m (×××•×¦×¢: {day['height']:.1f}m)")
    
    print("\nâœ… ×¡×™×•×!")
    print("\nğŸ’¡ ×”×¢×¨×”: ×”× ×ª×•× ×™× ××‘×•×¡×¡×™× ×¢×œ ×ª×—×–×™×•×ª Surfline.")
    print("   ××•××œ×¥ ×œ×‘×“×•×§ ×ª×—×–×™×•×ª ××¢×•×“×›× ×•×ª ×œ×¤× ×™ ×”×’×œ×™×©×”.")

if __name__ == "__main__":
    main()

